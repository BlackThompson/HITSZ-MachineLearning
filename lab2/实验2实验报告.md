# 1. 设备参数

- **GPU**: NVIDIA RTX 4090
- **操作系统**:  Ubuntu 22.04.2 LTS
- **Python 版本**: 3.9.0
- **PyTorch 版本**: 2.0.1

# 2. 实验结果



- TextCNN

![image-20231025193948850](https://black-thompson.oss-cn-beijing.aliyuncs.com/img/image-20231025193948850.png)

- TextRNN

![image-20231025194257265](https://black-thompson.oss-cn-beijing.aliyuncs.com/img/image-20231025194257265.png)

- TextRNN_Att

![image-20231025195055566](https://black-thompson.oss-cn-beijing.aliyuncs.com/img/image-20231025195055566.png)

- TextRCNN

![image-20231025195240953](https://black-thompson.oss-cn-beijing.aliyuncs.com/img/image-20231025195240953.png)

- FastText

![image-20231025195658571](https://black-thompson.oss-cn-beijing.aliyuncs.com/img/image-20231025195658571.png)

- DPCNN

![image-20231025195842114](https://black-thompson.oss-cn-beijing.aliyuncs.com/img/image-20231025195842114.png)

- Transformer

![image-20231025200044707](https://black-thompson.oss-cn-beijing.aliyuncs.com/img/image-20231025200044707.png)

# 3. 改进方案

```python
# 改进Transformer中的Attention块

import torch
from torch import nn, einsum
import torch.nn.functional as F

from einops import rearrange, repeat


# relative positional embedding

def to(x):
    return {'device': x.device, 'dtype': x.dtype}


def pair(x):
    return (x, x) if not isinstance(x, tuple) else x


def expand_dim(t, dim, k):
    t = t.unsqueeze(dim=dim)
    expand_shape = [-1] * len(t.shape)
    expand_shape[dim] = k
    return t.expand(*expand_shape)


def rel_to_abs(x):
    b, l, m = x.shape
    r = (m + 1) // 2

    col_pad = torch.zeros((b, l, 1), **to(x))
    x = torch.cat((x, col_pad), dim=2)
    flat_x = rearrange(x, 'b l c -> b (l c)')
    flat_pad = torch.zeros((b, m - l), **to(x))
    flat_x_padded = torch.cat((flat_x, flat_pad), dim=1)
    final_x = flat_x_padded.reshape(b, l + 1, m)
    final_x = final_x[:, :l, -r:]
    return final_x


def relative_logits_1d(q, rel_k):
    b, h, w, _ = q.shape
    r = (rel_k.shape[0] + 1) // 2

    logits = einsum('b x y d, r d -> b x y r', q, rel_k)
    logits = rearrange(logits, 'b x y r -> (b x) y r')
    logits = rel_to_abs(logits)

    logits = logits.reshape(b, h, w, r)
    logits = expand_dim(logits, dim=2, k=r)
    return logits


class RelPosEmb(nn.Module):
    def __init__(
            self,
            block_size,
            rel_size,
            dim_head
    ):
        super().__init__()
        height = width = rel_size
        scale = dim_head ** -0.5

        self.block_size = block_size
        self.rel_height = nn.Parameter(torch.randn(height * 2 - 1, dim_head) * scale)
        self.rel_width = nn.Parameter(torch.randn(width * 2 - 1, dim_head) * scale)

    def forward(self, q):
        block = self.block_size

        q = rearrange(q, 'b (x y) c -> b x y c', x=block)
        rel_logits_w = relative_logits_1d(q, self.rel_width)
        rel_logits_w = rearrange(rel_logits_w, 'b x i y j-> b (x y) (i j)')

        q = rearrange(q, 'b x y d -> b y x d')
        rel_logits_h = relative_logits_1d(q, self.rel_height)
        rel_logits_h = rearrange(rel_logits_h, 'b x i y j -> b (y x) (j i)')
        return rel_logits_w + rel_logits_h


# classes

class HaloAttention(nn.Module):
    def __init__(
            self,
            *,
            dim,
            block_size,
            halo_size,
            dim_head=64,
            heads=8
    ):
        super().__init__()
        assert halo_size > 0, 'halo size must be greater than 0'

        self.dim = dim
        self.heads = heads
        self.scale = dim_head ** -0.5

        self.block_size = block_size
        self.halo_size = halo_size

        inner_dim = dim_head * heads

        self.rel_pos_emb = RelPosEmb(
            block_size=block_size,
            rel_size=block_size + (halo_size * 2),
            dim_head=dim_head
        )

        self.to_q = nn.Linear(dim, inner_dim, bias=False)
        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)
        self.to_out = nn.Linear(inner_dim, dim)

    def forward(self, x):
        b, c, h, w, block, halo, heads, device = *x.shape, self.block_size, self.halo_size, self.heads, x.device
        assert h % block == 0 and w % block == 0, 'fmap dimensions must be divisible by the block size'
        assert c == self.dim, f'channels for input ({c}) does not equal to the correct dimension ({self.dim})'

        # get block neighborhoods, and prepare a halo-ed version (blocks with padding) for deriving key values

        q_inp = rearrange(x, 'b c (h p1) (w p2) -> (b h w) (p1 p2) c', p1=block, p2=block)

        kv_inp = F.unfold(x, kernel_size=block + halo * 2, stride=block, padding=halo)
        kv_inp = rearrange(kv_inp, 'b (c j) i -> (b i) j c', c=c)

        # derive queries, keys, values

        q = self.to_q(q_inp)
        k, v = self.to_kv(kv_inp).chunk(2, dim=-1)

        # split heads

        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=heads), (q, k, v))

        # scale

        q *= self.scale

        # attention

        sim = einsum('b i d, b j d -> b i j', q, k)

        # add relative positional bias

        sim += self.rel_pos_emb(q)

        # mask out padding (in the paper, they claim to not need masks, but what about padding?)

        mask = torch.ones(1, 1, h, w, device=device)
        mask = F.unfold(mask, kernel_size=block + (halo * 2), stride=block, padding=halo)
        mask = repeat(mask, '() j i -> (b i h) () j', b=b, h=heads)
        mask = mask.bool()

        max_neg_value = -torch.finfo(sim.dtype).max
        sim.masked_fill_(mask, max_neg_value)

        # attention

        attn = sim.softmax(dim=-1)

        # aggregate

        out = einsum('b i j, b j d -> b i d', attn, v)

        # merge and combine heads

        out = rearrange(out, '(b h) n d -> b n (h d)', h=heads)
        out = self.to_out(out)

        # merge blocks back to original feature map

        out = rearrange(out, '(b h w) (p1 p2) c -> b c (h p1) (w p2)', b=b, h=(h // block), w=(w // block), p1=block,
                        p2=block)
        return out


# 输入 N C H W,  输出 N C H W
if __name__ == '__main__':
    block = HaloAttention(dim=512,
                          block_size=2,
                          halo_size=1, ).cuda()
    input = torch.rand(1, 512, 64, 64).cuda()
    output = block(input)
    print(input.size(), output.size())

```

![image-20231025200933626](https://black-thompson.oss-cn-beijing.aliyuncs.com/img/image-20231025200933626.png)

1. 首先，导入了需要的PyTorch和einops库。einops库用于对张量进行操作和重排。

2. 接下来定义了一些辅助函数，如`to`、`pair`、`expand_dim`和`rel_to_abs`，这些函数用于在代码中进行张量操作和相对位置编码的处理。

3. `RelPosEmb` 类定义了相对位置编码的处理。它包括 `__init__` 方法，用于初始化相对位置编码的参数，以及 `forward` 方法，用于计算相对位置编码。相对位置编码是通过将输入张量的不同位置之间的相对距离编码成向量，以捕捉位置信息。

4. `HaloAttention` 类是主要的自注意力模块。它接受输入张量 `x`，其中 `dim` 表示输入特征的维度，`block_size` 表示块的大小，`halo_size` 表示Halo的大小，`dim_head` 表示每个注意头的维度，`heads` 表示注意力头的数量。在该类中，以下操作被执行：

   - 输入张量 `x` 被重排和裁剪以适应模型的输入要求，包括分割为块并为派生键值做准备。
   - 通过线性变换 `to_q`、`to_kv` 将输入特征映射为查询（query）、键（key）和值（value）。
   - 多头自注意力机制的计算，包括缩放操作和添加相对位置偏置。
   - 对注意力矩阵进行掩码操作，以排除填充的影响。
   - 最终的输出是通过多头自注意力机制的组合和线性变换获得的。

5. 最后的代码块是一个示例，创建了一个 `HaloAttention` 的实例，传入一个随机生成的输入张量 `input`，并计算输出。输出的大小将打印出来。

